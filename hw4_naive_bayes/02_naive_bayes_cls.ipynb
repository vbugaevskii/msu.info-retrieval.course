{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(filename):\n",
    "    with open(os.path.join(dirpath, filename)) as f_text:\n",
    "        return next(f_text)\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('data/politics/'):\n",
    "    docs_0 = [extract_text(f) for f in filenames]\n",
    "    \n",
    "for dirpath, dirnames, filenames in os.walk('data/sports/'):\n",
    "    docs_1 = [extract_text(f) for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_0), len(docs_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологическая предобработка предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ru_morph = MorphAnalyzer()\n",
    "en_morph = WordNetLemmatizer()\n",
    "\n",
    "CYRILLIC_PATTERN = re.compile('[а-яА-Я]')\n",
    "DIGIT_PATTERN = re.compile('\\d+')\n",
    "\n",
    "def has_cyrillic(text):\n",
    "    return bool(CYRILLIC_PATTERN.search(text))\n",
    "\n",
    "def has_numeric(text):\n",
    "    return bool(DIGIT_PATTERN.search(text))\n",
    "\n",
    "@lru_cache(maxsize=1500)\n",
    "def morph_process(token):\n",
    "    if has_cyrillic(token):\n",
    "        return ru_morph.parse(token)[0].normal_form\n",
    "    elif has_numeric(token):\n",
    "        return token\n",
    "    else:\n",
    "        return en_morph.lemmatize(token)\n",
    "    \n",
    "def morph_sentence(sentence):\n",
    "    res = re.findall('\\d+', sentence.lower())\n",
    "    words = re.findall('[^\\W\\d_]+', sentence.lower())\n",
    "    res.extend(map(morph_process, words))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class TfIdf:\n",
    "    def __init__(self, preprocess, stopwords=None, idf=True, norm=True):\n",
    "        self.preprocess_ = preprocess\n",
    "        self.vocabulary_ = {}\n",
    "        \n",
    "        self._use_idf = idf\n",
    "        self._use_norm = norm\n",
    "        \n",
    "    def _transform(self, docs, use_vocab=True):\n",
    "        if not use_vocab:\n",
    "            self.vocabulary_ = {}\n",
    "        vocabulary = self.vocabulary_\n",
    "        \n",
    "        df_ = {}\n",
    "        tf_, indices, indptr = [], [], [0]\n",
    "        \n",
    "        for doc_id, doc in enumerate(docs):\n",
    "            tf_counts, df_entries = {}, set()\n",
    "            \n",
    "            for term in self.preprocess_(doc):                \n",
    "                if term not in vocabulary:\n",
    "                    if use_vocab:\n",
    "                        continue\n",
    "                    vocabulary[term] = len(vocabulary)\n",
    "                term_id = vocabulary[term]\n",
    "                \n",
    "                if term_id not in tf_counts:\n",
    "                    tf_counts[term_id] = 0\n",
    "                tf_counts[term_id] += 1\n",
    "                \n",
    "                df_entries.add(term_id)\n",
    "                \n",
    "            indices.extend(tf_counts.keys())\n",
    "            tf_.extend(tf_counts.values())\n",
    "            indptr.append(len(indices))\n",
    "            \n",
    "            for term_id in df_entries:\n",
    "                df_[term_id] = df_.get(term_id, 0) + 1\n",
    "                \n",
    "        tf_ = csr_matrix((tf_, indices, indptr), dtype=np.uint32,\n",
    "                         shape=(len(docs), len(vocabulary)))\n",
    "        tf_.sort_indices()\n",
    "        tf_ = tf_.toarray()\n",
    "        \n",
    "        if self._use_norm:\n",
    "            # norm = np.sqrt(np.power(tf_, 2).sum(axis=1, keepdims=True))\n",
    "            norm = tf_.max(axis=1, keepdims=True)\n",
    "            tf_ = tf_ / norm\n",
    "        \n",
    "        if not use_vocab:\n",
    "            self.idf_ = [np.log10(len(docs) / float(df_[tid])) for tid in range(len(vocabulary))]\n",
    "            self.idf_ = np.asarray(self.idf_)\n",
    "        idf_ = self.idf_\n",
    "        \n",
    "        if self._use_idf:\n",
    "            tf_ = tf_ * idf_.reshape(1, -1)\n",
    "        \n",
    "        return tf_\n",
    "        \n",
    "    def fit_transform(self, docs):\n",
    "        return self._transform(docs, use_vocab=False)\n",
    "    \n",
    "    def fit(self, docs):\n",
    "        _ = self.fit_transform(docs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        return self._transform(docs, use_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self._ranker_tfidf = TfIdf(preprocess=morph_sentence, idf=False)\n",
    "    \n",
    "    def fit(self, docs, y):\n",
    "        assert len(docs) == len(y)\n",
    "        \n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # P(cl) ~ prior\n",
    "        cl_unique, cl_counts = np.unique(y, return_counts=True)\n",
    "        cl_counts = cl_counts / cl_counts.sum()\n",
    "        self.cl_probs = dict(zip(cl_unique, cl_counts))\n",
    "        \n",
    "        # P(term|cl) ~ likelihood\n",
    "        self.t_probs = {}\n",
    "        tfs = self._ranker_tfidf.fit_transform(docs)\n",
    "        for cl in self.cl_probs:\n",
    "            mask = y == cl\n",
    "            tf = 1 + tfs[mask, :].sum(axis=0)\n",
    "            self.t_probs[cl] = tf / tf.sum()\n",
    "            \n",
    "        self.cl_order = sorted(self.cl_probs.keys())\n",
    "        \n",
    "    def predict(self, docs):\n",
    "        result = []\n",
    "        tfs = self._ranker_tfidf.transform(docs)\n",
    "        \n",
    "        for doc_i in range(len(docs)):\n",
    "            mask = tfs[doc_i] > 0\n",
    "            prob = { cl: self.t_probs[cl][mask].prod() * prior\n",
    "                     for cl, prior in self.cl_probs.items() }\n",
    "            result.append(prob)\n",
    "            \n",
    "        result = np.asarray([[pred[cl] for cl in self.cl_order] for pred in result])\n",
    "        return result                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(X, n_train):\n",
    "    np.random.seed(8888)\n",
    "    index = np.random.permutation(len(X))\n",
    "    X_train = [X[i] for i in index[:n_train]]\n",
    "    X_valid = [X[i] for i in index[n_train:]]\n",
    "    return X_train, X_valid\n",
    "\n",
    "# класс 0 -- политика; класс 1 -- спорт\n",
    "docs_train_0, docs_valid_0 = train_valid_split(docs_0, 10)\n",
    "docs_train_1, docs_valid_1 = train_valid_split(docs_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayes()\n",
    "cl.fit(docs_train_0 + docs_train_1, [0] * 10 + [1] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ежегодное послание президента РФ Федеральному собранию, скорее всего, вновь прозвучит в начале следующего года, а не в декабре, как раньше. О вероятном переносе послания заявил пресс-секретарь президента Дмитрий Песков.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.21704514e-35, 4.25872451e-37]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs_valid_0[1])\n",
    "cl.predict([docs_valid_0[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нападающий \"Коламбуса\" Кэм Эткинсон стал первой звездой прошедшей недели в регулярном чемпионате НХЛ, сообщается на официальном сайте лиги. В трех матчах на минувшей неделе форвард набрал восемь очков (пять голов и три результативные передачи), в игре с \"Каролиной\" (4:1) оформил хет-трик.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.50306164e-61, 1.05017266e-57]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs_valid_1[5])\n",
    "cl.predict([docs_valid_1[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = cl.predict(docs_valid_0 + docs_valid_1)\n",
    "y_preds = np.argmax(y_preds, axis=1)\n",
    "\n",
    "y_true = [0] * len(docs_valid_0) + [1] * len(docs_valid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7759197324\n",
      "Recall:    0.7750000000\n",
      "F-measure: 0.7751355861\n"
     ]
    }
   ],
   "source": [
    "metrics = precision_recall_fscore_support(y_true, y_preds, average='macro')\n",
    "print((\"Precision: {:>.10f}\\n\" + \\\n",
    "       \"Recall:    {:>.10f}\\n\" + \\\n",
    "       \"F-measure: {:>.10f}\").format(*metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
